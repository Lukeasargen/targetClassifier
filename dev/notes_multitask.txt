

LiveClassifyDataset - no background
8.52 seconds with 0 workers. 0.53 seconds per 16 batches. 0.019 GB ram.
9.71 seconds with 1 workers. 0.61 seconds per 16 batches. 0.125 GB ram.
5.37 seconds with 2 workers. 0.34 seconds per 16 batches. 0.242 GB ram.
4.37 seconds with 3 workers. 0.27 seconds per 16 batches. 0.362 GB ram.
3.39 seconds with 4 workers. 0.21 seconds per 16 batches. 0.477 GB ram.
3.30 seconds with 5 workers. 0.21 seconds per 16 batches. 0.604 GB ram.
2.82 seconds with 6 workers. 0.18 seconds per 16 batches. 0.718 GB ram.
2.83 seconds with 7 workers. 0.18 seconds per 16 batches. 0.820 GB ram.
2.52 seconds with 8 workers. 0.16 seconds per 16 batches. 0.952 GB ram.

LiveClassifyDataset - background
11.50 seconds with 0 workers. 0.72 seconds per 16 batches. 0.007 GB ram.
12.59 seconds with 1 workers. 0.79 seconds per 16 batches. 0.326 GB ram.
7.95 seconds with 2 workers. 0.50 seconds per 16 batches. 0.643 GB ram.
7.72 seconds with 3 workers. 0.48 seconds per 16 batches. 0.968 GB ram.
7.40 seconds with 4 workers. 0.46 seconds per 16 batches. 1.291 GB ram.
8.31 seconds with 5 workers. 0.52 seconds per 16 batches. 1.607 GB ram.
8.78 seconds with 6 workers. 0.55 seconds per 16 batches. 1.922 GB ram.
9.79 seconds with 7 workers. 0.61 seconds per 16 batches. 2.226 GB ram.
10.45 seconds with 8 workers. 0.65 seconds per 16 batches. 2.528 GB ram.

FolderClassifyDataset
35.28 seconds with 0 workers. 0.28 seconds per 128 batches. 0.024 GB ram.
36.14 seconds with 1 workers. 0.28 seconds per 128 batches. 0.168 GB ram.
20.07 seconds with 2 workers. 0.16 seconds per 128 batches. 0.318 GB ram.
15.29 seconds with 3 workers. 0.12 seconds per 128 batches. 0.484 GB ram.
12.99 seconds with 4 workers. 0.10 seconds per 128 batches. 0.633 GB ram.
12.29 seconds with 5 workers. 0.10 seconds per 128 batches. 0.805 GB ram.
12.21 seconds with 6 workers. 0.10 seconds per 128 batches. 0.964 GB ram.
12.40 seconds with 7 workers. 0.10 seconds per 128 batches. 1.137 GB ram.
12.77 seconds with 8 workers. 0.10 seconds per 128 batches. 1.287 GB ram.


filters          blocks params   size   final accuracy                                                final loss sum
32, 32, 32 , 32  1,1,1  65395    0.25MB [73.46191406 94.8425293  64.43481445 97.81494141 89.31884766] 2.4543
16, 16, 32 , 64  1,1,1  84723    0.32MB [78.86962891 96.57592773 53.52172852 97.27172852 85.16235352] 2.8541
16, 32, 32 , 64  1,1,1  99699    0.38MB [82.00683594 96.75292969 67.56591797 97.91870117 88.43383789] 2.1554
32, 32, 64 , 64  1,1,1  164691   0.63MB [86.16333008 98.58398438 76.98364258 98.47412109 91.50390625] 1.5199
16, 16, 32 , 64  2,2,2  181939   0.69MB [86.41967773 98.81591797 76.3671875  97.8515625  89.64233398] 1.5326
16, 16, 32 , 64  2,3,2  200499   0.76MB [86.85913086 99.12719727 78.58276367 97.81494141 90.10620117] 1.4183
32, 32, 64 , 128 1,1,1  321939   1.23MB [88.65356445 99.25537109 83.41064453 98.46801758 92.54150391] 1.2128
32, 32, 64 , 64  2,2,2  331219   1.26MB [87.04223633 99.37133789 89.23339844 98.57177734 93.8293457 ] 0.9214
*32, 64, 64 , 128 1,1,1  378225   1.44MB [89.97991857 97.73671325 87.87549516 99.94016835 96.84852278] 0.7487
32, 64, 64 , 64  2,2,2  446291   1.70MB [89.99633789 99.70703125 91.02783203 98.99902344 95.01953125] 0.7379
16, 32, 64 , 128 2,2,2  703475   1.63MB [91.35131836 99.8046875  89.88647461 98.67553711 94.01245117] 0.7800
32, 32, 64 , 128 2,2,2  709907   2.71MB [91.68701172 99.63989258 91.07055664 98.77929688 94.44580078] 0.7324
32, 32, 64 , 128 2,3,2  783891   2.99MB [92.52319336 99.82910156 91.04614258 98.84643555 94.2565918 ] 0.6979
*32, 64, 64 , 128 2,2,2  824979   3.15MB [92.85888672 99.90234375 93.46923828 99.02954102 95.5078125 ] 0.5733
32, 32, 64 , 128 2,4,2  857875   3.27MB [92.87719727 99.79248047 91.69921875 98.66333008 94.59838867] 0.6640
*32, 64, 64 , 128 2,3,2  898693   3.43MB [93.32275391 99.86572266 94.16503906 99.0234375  95.72753906] 0.5375
32, 64, 128, 256 1,1,1  1233043  4.70MB [91.94946289 99.76196289 90.28320312 98.99902344 94.70825195] 0.7729
32, 64, 128, 256 2,2,2  2783091 10.61MB [93.99414062 99.8046875  97.41210938 99.75585938 96.82617188] 0.3334


/100
train data
f3x3s1p1a8          821,107 3.13MB [89.80102539 99.53613281 93.24951172 99.24316406 96.33789062] 0.5988
f3x3s1p1a8_weighted 821,107 3.13MB [89.88647461 99.48730469 93.62182617 99.31640625 96.74682617] 0.5826
f3x3s2p1a4          821,107 3.13MB [86.50512695 98.65722656 83.69750977 98.95019531 93.99414062] 1.0867
f3x3s2p1a4_weighted 821,107 3.13MB [86.39526367 98.75488281 83.28857422 98.87084961 94.17724609] 1.0881
f5x5s1p2a8          822,643 3.14MB [90.41137695 99.2980957  93.28613281 99.24926758 96.61254883] 0.5922
f5x5s1p2a8_weighted 822,643 3.14MB [90.20996094 99.48120117 93.07861328 99.27978516 96.66748047] 0.5731
f5x5s2p2a4          822,643 3.14MB [86.14501953 98.87695312 83.03222656 98.95629883 93.71337891] 1.1046
f5x5s2p2a4_weighted 822,643 3.14MB [85.34545898 98.7487793  81.45751953 98.87695312 93.72558594] 1.1753
f7x7s1p3a8          824,947 3.15MB [90.01464844 99.39575195 92.8894043  99.37744141 96.31347656] 0.6015
f7x7s1p3a8_weighted 824,947 3.15MB [90.33203125 99.51782227 93.21899414 99.35302734 96.42333984] 0.5845
f7x7s2p3a4          824,947 3.15MB [85.26000977 98.66943359 82.58666992 98.87084961 93.81713867] 1.1475
f7x7s2p3a4_weighted 824,947 3.15MB [83.93554688 98.83422852 82.20214844 98.81591797 93.70117188] 1.1872
val data
f3x3s1p1a8          821,107 3.13MB [93.359375   99.65820312 97.99804688 99.75585938 98.29101562] 0.3014
f3x3s1p1a8_weighted 821,107 3.13MB [93.79882812 99.70703125 98.2421875  99.8046875  98.2421875 ] 0.2608
f3x3s2p1a4          821,107 3.13MB [91.50390625 99.4140625  93.45703125 99.85351562 96.72851562] 0.5315
f3x3s2p1a4_weighted 821,107 3.13MB [91.84570312 99.56054688 93.06640625 99.609375   96.97265625] 0.5110
f5x5s1p2a8          822,643 3.14MB [93.9453125  99.70703125 98.38867188 99.85351562 97.8515625 ] 0.2755
f5x5s1p2a8_weighted 822,643 3.14MB [94.23828125 99.8046875  98.14453125 99.85351562 98.53515625] 0.2440
f5x5s2p2a4          822,643 3.14MB [91.84570312 99.70703125 93.26171875 99.70703125 96.43554688] 0.5338
f5x5s2p2a4_weighted 822,643 3.14MB [90.08789062 99.56054688 92.1875     99.65820312 95.94726562] 0.5940
f7x7s1p3a8          824,947 3.15MB [93.31054688 99.8046875  97.99804688 99.75585938 98.2421875 ] 0.2842
f7x7s1p3a8_weighted 824,947 3.15MB [93.359375   99.85351562 97.94921875 99.75585938 98.19335938] 0.2738
f7x7s2p3a4          824,947 3.15MB [90.0390625  99.51171875 92.87109375 99.51171875 96.24023438] 0.5835
f7x7s2p3a4_weighted 824,947 3.15MB [90.4296875  99.36523438 91.55273438 99.8046875  96.24023438] 0.6007

use f5 s1 1p

previous smallest: 16-18px depending on shape
smallest target: 17px
human readable: 18px no rotation, 22px rotation

classify1 [1, 1, 1] b128
train   [92.51608557 99.29470963 97.09841069 99.99243841 98.66434778]
val     [90.078125   98.59375    95.11160714 100.        97.90178571]

classify1 [1, 1, 1] b256
train   [91.9509243  99.11387269 96.44964514 99.98039998 98.47807548]
val     [89.67633929 98.33705357 94.27455357 99.96651786 97.890625  ]

classify1 [1, 1, 1] b512
train   [89.26778444 98.0571908  91.78931008 99.93810519 97.44511994]
val     [88.79825368 97.73667279 90.80882353 99.90808824 97.01286765]

classify1 [1, 1, 1] b256 stride=2 first conv
train   [89.97991857 97.73671325 87.87549516 99.94016835 96.84852278]
val     [87.68973214 96.55133929 83.63839286 99.89955357 95.44642857]

backgrounds live [1, 1, 1] b256
train   [89.46533203 98.18725586 93.37158203 99.97558594 97.57080078]
val     [89.01367188 98.73046875 93.60351562 99.90234375 97.70507812]

classify1 [2, 2, 2]
train   [91.88627861 99.31503081 95.94038842 99.96974032 98.25559804]
val     [90.89285714 99.12946429 95.         99.98883929 98.00223214]

classify1 [2, 3, 2]
weighted
train   [ 94.1199934   99.70049791  98.64037742 99.99449824   99.15272887]
val     [ 91.39508929  99.35267857  96.640625   100.          98.515625  ]
non weighted
train   [ 93.09116417  99.48455381  97.417955   99.98899648   98.80543024]
val     [ 91.43973214  99.19642857  95.97098214 100.          98.32589286]

classfy1 [32, 64, 128, 256] [1, 1, 1] b256
train   [94.87923636 99.70324879 98.68611081 99.99449824 99.12109375]
val     [90.78125    98.78348214 96.12723214 99.98883929 97.97991071]

classfy1 [32, 64, 128, 256] [2, 2, 2] b256
train   [95.34894916 99.7978103  99.26929743 99.99690526 99.33153609]
val     [91.83035714 99.4196428 97.32142857  100.        98.38169643]


classfy1 [16, 16, 32, 64] [1, 1, 1] b256 f3x3s1p1
train   [85.91755612 96.05283066 81.51442837 99.87517881 95.33588248]
val     [85.45758929 95.83705357 79.86607143 99.921875   95.17857143]

classfy1 [16, 16, 32, 64] [1, 1, 1] b256 f5x5s1p2 83,035 320kb
train   [86.22943717 96.12744828 83.73060629 99.82119278 95.19008583]
val     [85.546875   95.9375     82.12053571 99.78794643 94.38616071]

classfy1 [16, 16, 32, 64] [2, 1, 1] b256 f5x5s1p2 87,707 330kb
train   [86.78545885 96.27083792 85.17791318 99.85695423 95.62988281]
val     [86.19419643 95.88169643 83.40401786 99.83258929 95.37946429]

classfy1 [16, 16, 32, 64] [1, 2, 1] b256 f5x5s1p2 101,595 390kb
train   [87.9387654  97.33680403 87.25723482 99.87517881 95.43972821]
val     [87.76785714 96.875      85.61383929 99.921875   95.390625  ]

classfy1 [16, 16, 32, 64] [1, 1, 2] b256 f5x5s1p2 157,019 600kb
train   [88.79085057 97.6407763  88.07424626 99.89409111 96.24367298]
val     [87.87946429 96.99776786 85.65848214 99.89955357 95.75892857]

classfy1 [16, 16, 32, 64] [2, 2, 1] b256 f5x5s1p2 106,267 410kb
train   [88.31976232 97.83368178 89.19247909 99.89959287 96.29215724]
val     [87.46651786 97.58928571 87.47767857 99.89955357 95.73660714]

classfy1 [16, 16, 32, 64] [2, 1, 2] b256 f5x5s1p2 161,691 620kb
train   [89.43661972 98.03965394 89.95344135 99.94085607 96.26946248]
val     [88.24776786 97.80133929 88.16964286 99.96651786 95.73660714]

classfy1 [16, 16, 32, 64] [1, 2, 2] b256 f5x5s1p2 175,579 670kb
train   [89.63674626 98.37869993 90.53628411 99.92194377 96.61985585]
val     [89.140625   97.96875    88.68303571 99.91071429 95.78125   ]

classfy1 [16, 16, 32, 64] [2, 2, 2] b256 f5x5s1p2 180,251 690kb
train   [89.97854313 98.72118453 90.8392248  99.92469465 96.55314701]
val     [88.91741071 98.359375   89.01785714 99.88839286 96.10491071]

classify1 [16, 16, 32, 64] [2, 2, 2] LeakyReLU=0.2
train   [89.83446578 98.22086818 91.14422865 99.90131217 96.81551221]
val     [89.07366071 97.86830357 89.88839286 99.88839286 96.49553571]

classify1 [16, 64, 128, 256] [2, 2, 2] lr=1e-2 wd=0.0
train   [93.02961323 99.36145191 97.6493728  99.97902454 98.70674241]
val     [90.88169643 99.01785714 96.40625    99.94419643 98.32589286]

classify1 [16, 64, 128, 256] [2, 2, 2] lr=1e-2 wd=5e-4 
train   [92.90582361 99.30574659 97.52111301 99.98418244 98.68370379]
val     [91.171875   99.01785714 96.25       99.97767857 98.27008929]

classify1 [16, 64, 128, 256] [2, 2, 2] lr=1e-2 wd=5e-4 w bn safe
train   [93.05505887 99.4020274  97.77144311 99.98005612 98.73562665]
val     [91.08258929 98.92857143 96.76339286 99.98883929 98.30357143]



joint task  error, train_size=16384

classify0 orientation letter [32, 32, 64, 128] [2, 3, 2] wd=0 weighted
train   [3.81696429 2.37946429]
val     [15.58388158 15.81003289]

classify0 orientation letter [32, 32, 64, 128] [2, 3, 2] wd=5e-4 weighted
train   [3.19419643 1.64955357]
val     [14.96710526 13.34292763]

classify0 orientation letter [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted
train   [3.34375    1.55133929]
val     [14.92598684 13.40460526]

classify0 orientation letter [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=6e-2 ramp=18,2e-2, step=20,0.1,23
train   [5.62276786 3.31919643]
val     [13.21957237  9.49835526]




classify1 o [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [10.55994168]
val     [11.84151786]

classify1 s [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [1.45590339]
val     [1.54017857]

classify1 l [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [5.38244113]
val     [5.82589286]

classify1 sc [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [0.04917199]
val     [0.04464286]

classify1 lc [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [1.75815636]
val     [1.86383929]

classify1 ol [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [9.38290878 3.81134463]
val     [10.17857143  4.27455357]

classify1 llc [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [5.73627311 2.62158891]
val     [6.43973214 2.85714286]

classify1 ollc [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [10.57644696  5.08431448  2.89564536]
val     [11.47321429  6.12723214  3.32589286]

classify1 ssc [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [1.09485035 0.0481404 ]
val     [1.20535714 0.03348214]

classify1 sclc [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [0.0378246  1.79632482]
val     [0.01116071 1.97544643]

classify1 sl [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=5e-2 ramp=2,1e-2, step=5,0.1,7
train   [1.23308209 4.27933814]
val     [1.46205357 5.01116071]

classify1 osl [32, 32, 64, 128] [2, 3, 2] wd=5e-4 bn safe weighted lr=3e-2 ramp=2,1e-2, step=[5,6],0.1,7
train   [8.34617077 1.0126678  3.57614437]
val     [9.57589286 1.31696429 4.83258929]

live osl [32, 64, 128, 256] [2, 2, 2] wd=5e-4 bn safe weighted lr=8e-2 ramp=23,1e-2, step=[58,62],0.1,66
train live   [8.1237793  0.57983398 2.23388672]

live oslsclc [32, 64, 128, 256] [2, 2, 2] wd=5e-4 bn safe weighted lr=8e-2 ramp=23,1e-2, step=[58,62],0.1,66
train   [9.06982422 0.9765625  3.11279297 0.04272461 1.78833008]

live oslsclc [32, 64, 128, 256] [2, 3, 2] wd=5e-4 bn safe weighted lr=6e-2 ramp=23,1e-2, step=[62,68],0.1,72
train   [10.44921875  0.73852539  2.75268555  0.02441406  1.70898438]

live oslsclc [32, 32, 64, 128] [2, 2, 2] wd=5e-4 bn safe weighted lr=6e-2 ramp=23,1e-2, step=[62,68],0.1,72
train   [12.07275391  1.17797852  4.49829102  0.06103516  2.36816406] /1
train   [12.09106445  1.38549805  4.9621582   0.09155273  2.75268555] /10
train   [11.68212891  1.24511719  5.05981445  0.04882812  2.35595703] /20
train   [12.2253418   1.23291016  4.81567383  0.09155273  2.52075195] /100
train   [11.55395508  1.27563477  4.33349609  0.05493164  2.49633789] /400
train   [11.82861328  1.38549805  4.69360352  0.03662109  2.54516602] /1000
train   [11.77368164  1.14746094  4.79125977  0.07324219  2.30102539] /5000

live oslsclc [32, 32, 64, 128] [2, 2, 2] wd=5e-4 bn safe weighted lr=6e-2 ramp=23,1e-2, step=[62,68],0.1,72 in=48
train   [9.49707031 0.72021484 1.56860352 0.03051758 1.00097656]

live slsclc [32, 32, 64, 128] [2, 2, 2] wd=5e-4 bn safe weighted lr=6e-2 ramp=23,1e-2, step=[62,68],0.1,72 in=48
train   [0.67749023 1.37329102 0.03662109 0.81787109]

live slsclc [32, 32, 64, 128, 128] [1, 1, 1, 1] wd=5e-4 bn safe weighted lr=6e-2 ramp=23,1e-2, step=[62,68],0.1,72 in=64 62,0.72
train   [1.03149414 2.04467773 0.04272461 0.79345703]


live oslsclc [32, 32, 64, 128] [2, 2, 2] b512 wd=5e-4 bn safe weighted lr=6e-2 ramp=23,1e-2, step=[62,68],0.1,72 in=48
train   [17.1875      2.8137207  12.11547852  0.09765625  4.69970703]

live oslsclc [32, 32, 64, 128] [2, 2, 2] b64 wd=5e-4 bn safe weighted lr=6e-2 ramp=23,1e-2, step=[62,68],0.1,72 in=48
train   [15.49682617  2.27661133  7.99560547  0.52490234  5.10864258]





